import asyncio
import os
import logging
import xml.etree.ElementTree as ET
import hashlib
from typing import List, Dict
from pathlib import Path
from collections import defaultdict

import aiohttp
from openai import AsyncOpenAI
from pinecone import Pinecone
from sqlalchemy import select
from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker
from dotenv import load_dotenv

# === –ó–ê–ì–†–£–ó–ö–ê –ü–ï–†–ï–ú–ï–ù–ù–´–• ===
BASE_DIR = Path(__file__).resolve().parent.parent
env_path = BASE_DIR / ".env"
load_dotenv(dotenv_path=env_path)

DATABASE_URL = os.getenv("DB_URL")
if not DATABASE_URL:
    DATABASE_URL = os.getenv("DATABASE_URL")
if not DATABASE_URL:
    raise ValueError(f"‚ùå –û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è DB_URL –≤ {env_path}")

if "sqlite" in DATABASE_URL and "aiosqlite" not in DATABASE_URL:
    DATABASE_URL = DATABASE_URL.replace("sqlite://", "sqlite+aiosqlite://")

import sys

sys.path.append(str(BASE_DIR))
from app.db.models import Magazine

# === –ù–ê–°–¢–†–û–ô–ö–ò ===
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME", "strollers-index")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
pc = Pinecone(api_key=PINECONE_API_KEY)
index = pc.Index(PINECONE_INDEX_NAME)


async def download_feed(session: aiohttp.ClientSession, url: str) -> str:
    try:
        async with session.get(url, timeout=60) as response:
            if response.status == 200:
                return await response.text()
            else:
                logger.error(f"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Ñ–∏–¥–∞ {url}: Status {response.status}")
                return None
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å {url}: {e}")
        return None


def parse_offers_from_xml(xml_content: str) -> List[Dict]:
    products = []
    try:
        root = ET.fromstring(xml_content)
        for offer in root.findall(".//offer"):
            available = offer.get("available")
            if available == "false":
                continue

            name = offer.findtext("name") or offer.findtext("model")
            raw_description = offer.findtext("description") or ""
            url = offer.findtext("url")
            price = offer.findtext("price")
            vendor = offer.findtext("vendor") or ""

            # --- üî• –ù–û–í–û–ï: –°–±–æ—Ä —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∏–∑ —Ç–µ–≥–æ–≤ <param> ---
            params_list = []
            for param in offer.findall("param"):
                p_name = param.get("name")
                p_value = param.text
                if p_name and p_value:
                    params_list.append(f"{p_name}: {p_value}")

            # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫—É —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            params_str = "; ".join(params_list)

            # –§–æ—Ä–º–∏—Ä—É–µ–º "–£–º–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ" –¥–ª—è AI
            # –°–Ω–∞—á–∞–ª–∞ —Ñ–∞–∫—Ç—ã (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã), –ø–æ—Ç–æ–º –ª–∏—Ä–∏–∫–∞ (–æ–ø–∏—Å–∞–Ω–∏–µ)
            # AI (Gemini) –æ–±–æ–∂–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –Ω–∞—á–∞–ª–µ.
            full_description = f"–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: {params_str}. –û–ø–∏—Å–∞–Ω–∏–µ: {raw_description}"
            # --- –ö–û–ù–ï–¶ –ë–õ–û–ö–ê ---

            # –¢–µ–∫—Å—Ç –¥–ª—è –í–µ–∫—Ç–æ—Ä–∞ (OpenAI Embeddings)
            # –¢–µ–ø–µ—Ä—å –ø–æ–∏—Å–∫ –±—É–¥–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å "–ª–µ–≥–∫—É—é –∫–æ–ª—è—Å–∫—É", –ø–æ—Ç–æ–º—É —á—Ç–æ "–í–µ—Å: 10–∫–≥" –µ—Å—Ç—å –≤ –≤–µ–∫—Ç–æ—Ä–µ
            full_text_for_search = f"{name} {vendor} {params_str} {raw_description} –¶–µ–Ω–∞: {price}".strip()

            if name and url:
                products.append({
                    "id": offer.get("id"),
                    "text": full_text_for_search,
                    "metadata": {
                        "name": name,
                        "url": url,
                        "price": price,
                        # –û–±—Ä–µ–∑–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ –¥–æ 3000 —Å–∏–º–≤–æ–ª–æ–≤
                        "description": full_description[:3000]
                    }
                })
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML: {e}")

    return products


async def get_embeddings_batch(texts: List[str]) -> List[List[float]]:
    try:
        response = await openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=texts
        )
        return [data.embedding for data in response.data]
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ OpenAI Embeddings: {e}")
        return []


async def process_feed_group(session: aiohttp.ClientSession, feed_url: str, magazines: List[Magazine]):
    mag_names = [m.name for m in magazines]
    # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: Pinecone —Ç—Ä–µ–±—É–µ—Ç —Å–ø–∏—Å–æ–∫ –°–¢–†–û–ö, –∞ –Ω–µ —á–∏—Å–µ–ª
    mag_ids = [str(m.id) for m in magazines]

    logger.info(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø—ã –º–∞–≥–∞–∑–∏–Ω–æ–≤: {mag_names}")

    xml_content = await download_feed(session, feed_url)
    if not xml_content: return

    products = parse_offers_from_xml(xml_content)
    logger.info(f"üì¶ –í —Ñ–∏–¥–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)}")

    if not products: return

    batch_size = 100
    for i in range(0, len(products), batch_size):
        batch = products[i: i + batch_size]
        texts_to_embed = [p["text"] for p in batch]
        embeddings = await get_embeddings_batch(texts_to_embed)

        if not embeddings: continue

        vectors_to_upsert = []
        url_hash = hashlib.md5(feed_url.encode()).hexdigest()[:10]

        for j, product in enumerate(batch):
            vector_id = f"feed_{url_hash}_{product['id']}"
            metadata = product["metadata"]
            metadata["magazine_ids"] = mag_ids  # –°–ø–∏—Å–æ–∫ ID

            vectors_to_upsert.append({
                "id": vector_id,
                "values": embeddings[j],
                "metadata": metadata
            })

        try:
            index.upsert(vectors=vectors_to_upsert)
            logger.info(f"‚úÖ –ì—Ä—É–ø–ø–∞ {mag_names}: –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(vectors_to_upsert)} —Ç–æ–≤–∞—Ä–æ–≤...")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ Pinecone Upsert: {e}")

    logger.info(f"üéâ –ì—Ä—É–ø–ø–∞ {mag_names} –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–±–Ω–æ–≤–ª–µ–Ω–∞!")


async def run_update_cycle():
    """–û–¥–∏–Ω –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"""
    logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã —Ç–æ–≤–∞—Ä–æ–≤...")

    engine = create_async_engine(DATABASE_URL)
    async_session = async_sessionmaker(engine, expire_on_commit=False)

    async with async_session() as db_session:
        result = await db_session.execute(select(Magazine))
        all_magazines = result.scalars().all()

        feed_groups = defaultdict(list)
        for mag in all_magazines:
            raw_url = mag.feed_url
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL –µ—Å—Ç—å –∏ —ç—Ç–æ –Ω–µ Google_Search
            if raw_url and raw_url.strip() != "Google_Search":
                # üî• –ì–õ–ê–í–ù–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –ø–æ –∫—Ä–∞—è–º
                clean_url = raw_url.strip()
                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —á–∏—Å—Ç–æ–π —Å—Å—ã–ª–∫–µ
                feed_groups[clean_url].append(mag)
            else:
                logger.info(f"‚è≠ –ú–∞–≥–∞–∑–∏–Ω {mag.name} –ø—Ä–æ–ø—É—â–µ–Ω")

        async with aiohttp.ClientSession() as http_session:
            for feed_url, mags_in_group in feed_groups.items():
                await process_feed_group(http_session, feed_url, mags_in_group)

    await engine.dispose()
    logger.info("üèÅ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")


if __name__ == "__main__":
    # –ü—Ä–æ—Å—Ç–æ –∑–∞–ø—É—Å–∫–∞–µ–º –æ–¥–∏–Ω —Ä–∞–∑ –∏ –≤—ã—Ö–æ–¥–∏–º
    asyncio.run(run_update_cycle())














#import asyncio
# import os
# import logging
# import xml.etree.ElementTree as ET
# from typing import List, Dict
# from pathlib import Path
#
# import aiohttp
# from openai import AsyncOpenAI
# from pinecone import Pinecone, ServerlessSpec
# from sqlalchemy import select
# from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
# from dotenv import load_dotenv
#
# # === –ó–ê–ì–†–£–ó–ö–ê –ü–ï–†–ï–ú–ï–ù–ù–´–• –û–ö–†–£–ñ–ï–ù–ò–Ø ===
# # 1. –ù–∞—Ö–æ–¥–∏–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É .env (–æ–Ω –Ω–∞ —É—Ä–æ–≤–µ–Ω—å –≤—ã—à–µ –ø–∞–ø–∫–∏ app)
# BASE_DIR = Path(__file__).resolve().parent.parent
# env_path = BASE_DIR / ".env"
#
# # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–º–µ–Ω–Ω–æ –∏–∑ —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
# load_dotenv(dotenv_path=env_path)
#
# # === –ü–†–û–í–ï–†–ö–ê ===
# DATABASE_URL = os.getenv("DB_URL")
# if not DATABASE_URL:
#     raise ValueError(f"‚ùå –û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è DATABASE_URL. –£–±–µ–¥–∏—Å—å, —á—Ç–æ –æ–Ω–∞ –µ—Å—Ç—å –≤ —Ñ–∞–π–ª–µ {env_path}")
#
# # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è SQLite, –Ω—É–∂–Ω–æ —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –¥—Ä–∞–π–≤–µ—Ä –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π
# if "sqlite" in DATABASE_URL and "aiosqlite" not in DATABASE_URL:
#     # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º sqlite:// –Ω–∞ sqlite+aiosqlite://
#     DATABASE_URL = DATABASE_URL.replace("sqlite://", "sqlite+aiosqlite://")
#
# # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–≤–æ–∏ –º–æ–¥–µ–ª–∏
# # (–ß—Ç–æ–±—ã Python —É–≤–∏–¥–µ–ª –ø–∞–ø–∫—É app, –∏–Ω–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –µ–µ –≤ –ø—É—Ç—å, –Ω–æ –ø–æ–ø—Ä–æ–±—É–µ–º –ø–æ–∫–∞ —Ç–∞–∫)
# import sys
# sys.path.append(str(BASE_DIR)) # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç–∏ –ø–æ–∏—Å–∫–∞
# from app.db.models import Magazine, Base
#
#
# load_dotenv()
# # === –ù–ê–°–¢–†–û–ô–ö–ò ===
# # –°—é–¥–∞ –ø–æ–¥—Ç—è–Ω—É—Ç—Å—è –¥–∞–Ω–Ω—ã–µ –∏–∑ .env —Ñ–∞–π–ª–∞
# PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
# PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME", "strollers-index")
# OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# # –°—Ç—Ä–æ–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ —Ç–≤–æ–µ–π –ë–î (–∫–∞–∫ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –±–æ—Ç–µ)
# DATABASE_URL = os.getenv("DB_URL")
#
# # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (—á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –≤ –∫–æ–Ω—Å–æ–ª–∏)
# logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
# logger = logging.getLogger(__name__)
#
# # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤
# openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
# pc = Pinecone(api_key=PINECONE_API_KEY)
# index = pc.Index(PINECONE_INDEX_NAME)
#
#
# async def download_feed(session: aiohttp.ClientSession, url: str) -> str:
#     """–°–∫–∞—á–∏–≤–∞–µ—Ç YML/XML —Ñ–∞–π–ª –º–∞–≥–∞–∑–∏–Ω–∞"""
#     try:
#         async with session.get(url, timeout=60) as response:
#             if response.status == 200:
#                 return await response.text()
#             else:
#                 logger.error(f"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Ñ–∏–¥–∞ {url}: Status {response.status}")
#                 return None
#     except Exception as e:
#         logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å {url}: {e}")
#         return None
#
#
# def parse_offers_from_xml(xml_content: str) -> List[Dict]:
#     """–†–∞–∑–±–∏—Ä–∞–µ—Ç XML –∏ –¥–æ—Å—Ç–∞–µ—Ç —Ç–æ–≤–∞—Ä—ã (–ù–∞–∑–≤–∞–Ω–∏–µ, –û–ø–∏—Å–∞–Ω–∏–µ, –°—Å—ã–ª–∫–∞, –¶–µ–Ω–∞)"""
#     products = []
#     try:
#         root = ET.fromstring(xml_content)
#         # –í YML —Ç–æ–≤–∞—Ä—ã –æ–±—ã—á–Ω–æ –ª–µ–∂–∞—Ç –≤ shop -> offers -> offer
#         # –ò—â–µ–º –≤—Å–µ —Ç–µ–≥–∏ 'offer'
#         for offer in root.findall(".//offer"):
#             # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ (available="true")
#             available = offer.get("available")
#             if available == "false":
#                 continue
#
#             name = offer.findtext("name") or offer.findtext("model")
#             description = offer.findtext("description") or ""
#             url = offer.findtext("url")
#             price = offer.findtext("price")
#
#             # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∞: –ù–∞–∑–≤–∞–Ω–∏–µ + –û–ø–∏—Å–∞–Ω–∏–µ + –¶–µ–Ω–∞
#             # –≠—Ç–æ —Ç–æ, –ø–æ —á–µ–º—É –ò–ò –±—É–¥–µ—Ç –∏—Å–∫–∞—Ç—å "—Å–º—ã—Å–ª"
#             full_text_for_search = f"{name} {description} –¶–µ–Ω–∞: {price}".strip()
#
#             if name and url:
#                 products.append({
#                     "id": offer.get("id"),  # ID —Ç–æ–≤–∞—Ä–∞ –≤ –º–∞–≥–∞–∑–∏–Ω–µ
#                     "text": full_text_for_search,  # –¢–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
#                     "metadata": {
#                         "name": name,
#                         "url": url,
#                         "price": price,
#                         "description": description[:1000]  # –û–±—Ä–µ–∑–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ
#                     }
#                 })
#     except Exception as e:
#         logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML: {e}")
#
#     return products
#
#
# async def get_embeddings_batch(texts: List[str]) -> List[List[float]]:
#     """–ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –≤ —Å–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤ (Batching)"""
#     try:
#         # text-embedding-3-small - –¥–µ—à–µ–≤–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å
#         response = await openai_client.embeddings.create(
#             model="text-embedding-3-small",
#             input=texts
#         )
#         # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤
#         return [data.embedding for data in response.data]
#     except Exception as e:
#         logger.error(f"–û—à–∏–±–∫–∞ OpenAI Embeddings: {e}")
#         return []
#
#
# async def process_magazine(session: aiohttp.ClientSession, magazine: Magazine):
#     """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ –º–∞–≥–∞–∑–∏–Ω–∞"""
#     # –ë–´–õ–û: magazine.title -> –°–¢–ê–õ–û: magazine.name
#     logger.info(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–∞–≥–∞–∑–∏–Ω–∞: {magazine.name} (ID: {magazine.id})")
#
#     if not magazine.feed_url:
#         # –ë–´–õ–û: magazine.title -> –°–¢–ê–õ–û: magazine.name
#         logger.info(f"‚ö†Ô∏è –£ –º–∞–≥–∞–∑–∏–Ω–∞ {magazine.name} –Ω–µ—Ç YML-—Ñ–∏–¥–∞. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
#         return
#
#     # 1. –°–∫–∞—á–∏–≤–∞–µ–º
#     xml_content = await download_feed(session, magazine.feed_url)
#     if not xml_content:
#         return
#
#     # 2. –ü–∞—Ä—Å–∏–º
#     products = parse_offers_from_xml(xml_content)
#     logger.info(f"üì¶ –ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)}")
#
#     if not products:
#         return
#
#     # 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä—ã –∏ –≥—Ä—É–∑–∏–º –≤ Pinecone –ü–ê–ß–ö–ê–ú–ò –ø–æ 100 —à—Ç—É–∫
#     batch_size = 100
#
#     for i in range(0, len(products), batch_size):
#         batch = products[i: i + batch_size]
#
#         # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è OpenAI
#         texts_to_embed = [p["text"] for p in batch]
#
#         # –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã
#         embeddings = await get_embeddings_batch(texts_to_embed)
#
#         if not embeddings:
#             continue
#
#         # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Pinecone
#         vectors_to_upsert = []
#         for j, product in enumerate(batch):
#             vector_id = f"mag_{magazine.id}_{product['id']}"
#
#             metadata = product["metadata"]
#             metadata["magazine_id"] = magazine.id
#
#             vectors_to_upsert.append({
#                 "id": vector_id,
#                 "values": embeddings[j],
#                 "metadata": metadata
#             })
#
#         # –ó–∞–≥—Ä—É–∑–∫–∞ –≤ Pinecone
#         try:
#             index.upsert(vectors=vectors_to_upsert)
#             logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(vectors_to_upsert)} —Ç–æ–≤–∞—Ä–æ–≤...")
#         except Exception as e:
#             logger.error(f"–û—à–∏–±–∫–∞ Pinecone Upsert: {e}")
#
#     # –ë–´–õ–û: magazine.title -> –°–¢–ê–õ–û: magazine.name
#     logger.info(f"üéâ –ú–∞–≥–∞–∑–∏–Ω {magazine.name} –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≥—Ä—É–∂–µ–Ω!")
#
#
# async def main():
#     # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î (SQLite –∏–ª–∏ PostgreSQL - –≤–æ–∑—å–º–µ—Ç –∏–∑ URL)
#     engine = create_async_engine(DATABASE_URL)
#     async_session = async_sessionmaker(engine, expire_on_commit=False)
#
#     async with aiohttp.ClientSession() as http_session:
#         async with async_session() as db_session:
#             # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –º–∞–≥–∞–∑–∏–Ω—ã
#             result = await db_session.execute(select(Magazine))
#             magazines = result.scalars().all()
#
#             for magazine in magazines:
#                 await process_magazine(http_session, magazine)
#
#     await engine.dispose()
#     logger.info("üèÅ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
#
#
# if __name__ == "__main__":
#     # –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞
#     try:
#         asyncio.run(main())
#     except KeyboardInterrupt:
#         pass