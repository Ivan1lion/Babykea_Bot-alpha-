import asyncio
import os
import logging
import xml.etree.ElementTree as ET
import hashlib
from typing import List, Dict
from pathlib import Path
from collections import defaultdict

import aiohttp
import chromadb
from openai import AsyncOpenAI
from sqlalchemy import select
from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker
from dotenv import load_dotenv

# === –ó–ê–ì–†–£–ó–ö–ê –ü–ï–†–ï–ú–ï–ù–ù–´–• ===
BASE_DIR = Path(__file__).resolve().parent.parent
env_path = BASE_DIR / ".env"
load_dotenv(dotenv_path=env_path)

DATABASE_URL = os.getenv("DB_URL")
if not DATABASE_URL:
    DATABASE_URL = os.getenv("DATABASE_URL")
if not DATABASE_URL:
    raise ValueError(f"‚ùå –û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è DB_URL –≤ {env_path}")

if "sqlite" in DATABASE_URL and "aiosqlite" not in DATABASE_URL:
    DATABASE_URL = DATABASE_URL.replace("sqlite://", "sqlite+aiosqlite://")

import sys

sys.path.append(str(BASE_DIR))
from app.db.models import Magazine

# === –ù–ê–°–¢–†–û–ô–ö–ò ===
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)

# === üî• –ù–ê–°–¢–†–û–ô–ö–ê CHROMA DB ===
# –£–∫–∞–∑—ã–≤–∞–µ–º –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ, –≥–¥–µ –±—É–¥—É—Ç –ª–µ–∂–∞—Ç—å —Ñ–∞–π–ª—ã –±–∞–∑—ã
CHROMA_DB_PATH = os.path.join(BASE_DIR, "chromadb_storage")

# –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç (PersistentClient —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ –¥–∏—Å–∫)
chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)

# –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é (–∞–Ω–∞–ª–æ–≥ Index –≤ Pinecone)
# metadata={"hnsw:space": "cosine"} –≥–æ–≤–æ—Ä–∏—Ç –±–∞–∑–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (–∫–∞–∫ –≤ Pinecone)
collection = chroma_client.get_or_create_collection(name="strollers", metadata={"hnsw:space": "cosine"})


async def download_feed(session: aiohttp.ClientSession, url: str) -> str:
    try:
        async with session.get(url, timeout=60) as response:
            if response.status == 200:
                return await response.text()
            else:
                logger.error(f"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Ñ–∏–¥–∞ {url}: Status {response.status}")
                return None
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å {url}: {e}")
        return None


def parse_offers_from_xml(xml_content: str) -> List[Dict]:
    products = []
    try:
        root = ET.fromstring(xml_content)
        for offer in root.findall(".//offer"):
            available = offer.get("available")
            if available == "false":
                continue

            name = offer.findtext("name") or offer.findtext("model")
            # –í–∞–∂–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É–µ–º raw_description, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –æ—à–∏–±–∫–∏ –∏–º–µ–Ω–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            raw_description = offer.findtext("description") or ""
            url = offer.findtext("url")
            price = offer.findtext("price")
            vendor = offer.findtext("vendor") or ""

            # --- –°–±–æ—Ä —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∏–∑ —Ç–µ–≥–æ–≤ <param> ---
            params_list = []
            for param in offer.findall("param"):
                p_name = param.get("name")
                p_value = param.text
                if p_name and p_value:
                    params_list.append(f"{p_name}: {p_value}")

            # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫—É —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            params_str = "; ".join(params_list)

            # –§–æ—Ä–º–∏—Ä—É–µ–º "–£–º–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ" –¥–ª—è AI
            # –°–Ω–∞—á–∞–ª–∞ —Ñ–∞–∫—Ç—ã (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã), –ø–æ—Ç–æ–º –ª–∏—Ä–∏–∫–∞ (–æ–ø–∏—Å–∞–Ω–∏–µ)
            full_description = f"–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: {params_str}. –û–ø–∏—Å–∞–Ω–∏–µ: {raw_description}"
            # --- –ö–û–ù–ï–¶ –ë–õ–û–ö–ê ---

            # –¢–µ–∫—Å—Ç –¥–ª—è –í–µ–∫—Ç–æ—Ä–∞
            full_text_for_search = f"{name} {vendor} {params_str} {raw_description} –¶–µ–Ω–∞: {price}".strip()

            if name and url:
                products.append({
                    "id": offer.get("id"),
                    "text": full_text_for_search,
                    "metadata": {
                        "name": name,
                        "url": url,
                        "price": price,
                        # –û–±—Ä–µ–∑–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –¥–æ 3000 —Å–∏–º–≤–æ–ª–æ–≤ (—Ç–≤–æ–π –Ω–æ–≤—ã–π –ª–∏–º–∏—Ç)
                        "description": full_description[:3000]
                    }
                })
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML: {e}")

    return products


async def get_embeddings_batch(texts: List[str]) -> List[List[float]]:
    try:
        response = await openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=texts
        )
        return [data.embedding for data in response.data]
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ OpenAI Embeddings: {e}")
        return []


async def process_feed_group(session: aiohttp.ClientSession, feed_url: str, magazines: List[Magazine]):
    mag_names = [m.name for m in magazines]
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º ID –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏ (Chroma –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–æ —Å—Ç—Ä–æ–∫–∞–º–∏ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö)
    mag_ids = [str(m.id) for m in magazines]

    # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ ID –≤ —Å—Ç—Ä–æ–∫—É "1,2,5"
    mag_ids_str = ",".join(mag_ids)

    logger.info(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø—ã –º–∞–≥–∞–∑–∏–Ω–æ–≤: {mag_names}")

    xml_content = await download_feed(session, feed_url)
    if not xml_content: return

    products = parse_offers_from_xml(xml_content)
    logger.info(f"üì¶ –í —Ñ–∏–¥–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)}")

    if not products: return

    batch_size = 100
    for i in range(0, len(products), batch_size):
        batch = products[i: i + batch_size]
        texts_to_embed = [p["text"] for p in batch]
        embeddings = await get_embeddings_batch(texts_to_embed)

        if not embeddings: continue

        # === üî• –ò–ó–ú–ï–ù–ï–ù–ò–Ø –ü–û–î CHROMADB ===
        ids_batch = []
        embeddings_batch = []
        metadatas_batch = []
        documents_batch = []

        url_hash = hashlib.md5(feed_url.encode()).hexdigest()[:10]

        for j, product in enumerate(batch):
            vector_id = f"feed_{url_hash}_{product['id']}"

            meta = product["metadata"]

            # üî• –í–ê–ñ–ù–û–ï –î–û–ë–ê–í–õ–ï–ù–ò–ï: –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É-–∏—Å—Ç–æ—á–Ω–∏–∫
            # –≠—Ç–æ –Ω—É–∂–Ω–æ –¥–ª—è manage_chroma.py, —á—Ç–æ–±—ã —É–¥–∞–ª—è—Ç—å —Ñ–∏–¥—ã —Ü–µ–ª–∏–∫–æ–º
            meta["source_url"] = feed_url

            # –î–æ–±–∞–≤–ª—è–µ–º ID –º–∞–≥–∞–∑–∏–Ω–æ–≤ (–¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ "—Å–≤–æ–π-—á—É–∂–æ–π")
            meta["magazine_ids_str"] = mag_ids_str

            ids_batch.append(vector_id)
            embeddings_batch.append(embeddings[j])
            metadatas_batch.append(meta)
            documents_batch.append(product["text"])

        try:
            # Upsert –≤ Chroma
            collection.upsert(
                ids=ids_batch,
                embeddings=embeddings_batch,
                metadatas=metadatas_batch,
                documents=documents_batch
            )
            logger.info(f"‚úÖ –ì—Ä—É–ø–ø–∞ {mag_names}: –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(ids_batch)} —Ç–æ–≤–∞—Ä–æ–≤...")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ ChromaDB Upsert: {e}")

    logger.info(f"üéâ –ì—Ä—É–ø–ø–∞ {mag_names} –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–±–Ω–æ–≤–ª–µ–Ω–∞!")

# async def process_feed_group(session: aiohttp.ClientSession, feed_url: str, magazines: List[Magazine]):
#     mag_names = [m.name for m in magazines]
#     # –°–æ—Ö—Ä–∞–Ω—è–µ–º ID –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏ (Chroma –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–æ —Å—Ç—Ä–æ–∫–∞–º–∏ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö)
#     mag_ids = [str(m.id) for m in magazines]
#     # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ ID –≤ —Å—Ç—Ä–æ–∫—É "1,2,5", —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å —Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–ø–∏—Å–∫–æ–≤ –≤ —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏—è—Ö
#     # –ù–æ Chroma 0.4+ —É–º–µ–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å —Å–ø–∏—Å–∫–∏. –û—Å—Ç–∞–≤–∏–º –ø–æ–∫–∞ —Ç–∞–∫, –Ω–æ –¥–æ–±–∞–≤–∏–º –ø–æ–ª–µ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞.
#     mag_ids_str = ",".join(mag_ids)
#
#     logger.info(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø—ã –º–∞–≥–∞–∑–∏–Ω–æ–≤: {mag_names}")
#
#     xml_content = await download_feed(session, feed_url)
#     if not xml_content: return
#
#     products = parse_offers_from_xml(xml_content)
#     logger.info(f"üì¶ –í —Ñ–∏–¥–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)}")
#
#     if not products: return
#
#     batch_size = 100
#     for i in range(0, len(products), batch_size):
#         batch = products[i: i + batch_size]
#         texts_to_embed = [p["text"] for p in batch]
#         embeddings = await get_embeddings_batch(texts_to_embed)
#
#         if not embeddings: continue
#
#         # === üî• –ò–ó–ú–ï–ù–ï–ù–ò–Ø –ü–û–î CHROMADB ===
#         # Chroma –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–∫–∏ –∫–æ–ª–æ–Ω–æ–∫, –∞ –Ω–µ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
#         ids_batch = []
#         embeddings_batch = []
#         metadatas_batch = []
#         documents_batch = []
#
#         url_hash = hashlib.md5(feed_url.encode()).hexdigest()[:10]
#
#         for j, product in enumerate(batch):
#             vector_id = f"feed_{url_hash}_{product['id']}"
#
#             meta = product["metadata"]
#             # –î–æ–±–∞–≤–ª—è–µ–º ID –º–∞–≥–∞–∑–∏–Ω–æ–≤ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ.
#             # –•—Ä–∞–Ω–∏–º –∫–∞–∫ —Å—Ç—Ä–æ–∫—É –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è, –Ω–æ Chroma –ø–æ–π–º–µ—Ç –∏ —Ç–∞–∫.
#             # –í–∞–∂–Ω–æ: –ß—Ç–æ–±—ã —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å, –Ω–∞–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –∏—Å–∫–∞—Ç—å –≤—Ö–æ–∂–¥–µ–Ω–∏–µ.
#             # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–∏–º mag_ids_str, –∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –±—É–¥–µ–º –≤ Python-–∫–æ–¥–µ –ø–æ—Å–ª–µ –ø–æ–∏—Å–∫–∞ (—Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç)
#             meta["magazine_ids_str"] = mag_ids_str
#
#             ids_batch.append(vector_id)
#             embeddings_batch.append(embeddings[j])
#             metadatas_batch.append(meta)
#             documents_batch.append(product["text"])
#
#         try:
#             # Upsert –≤ Chroma
#             collection.upsert(
#                 ids=ids_batch,
#                 embeddings=embeddings_batch,
#                 metadatas=metadatas_batch,
#                 documents=documents_batch
#             )
#             logger.info(f"‚úÖ –ì—Ä—É–ø–ø–∞ {mag_names}: –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(ids_batch)} —Ç–æ–≤–∞—Ä–æ–≤...")
#         except Exception as e:
#             logger.error(f"–û—à–∏–±–∫–∞ ChromaDB Upsert: {e}")
#
#     logger.info(f"üéâ –ì—Ä—É–ø–ø–∞ {mag_names} –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–±–Ω–æ–≤–ª–µ–Ω–∞!")


async def run_update_cycle():
    """–û–¥–∏–Ω –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"""
    logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã —Ç–æ–≤–∞—Ä–æ–≤ (ChromaDB)...")

    engine = create_async_engine(DATABASE_URL)
    async_session = async_sessionmaker(engine, expire_on_commit=False)

    async with async_session() as db_session:
        result = await db_session.execute(select(Magazine))
        all_magazines = result.scalars().all()

        feed_groups = defaultdict(list)
        for mag in all_magazines:
            raw_url = mag.feed_url
            if raw_url and raw_url.strip() != "Google_Search":
                clean_url = raw_url.strip()
                feed_groups[clean_url].append(mag)
            else:
                logger.info(f"‚è≠ –ú–∞–≥–∞–∑–∏–Ω {mag.name} –ø—Ä–æ–ø—É—â–µ–Ω")

        async with aiohttp.ClientSession() as http_session:
            for feed_url, mags_in_group in feed_groups.items():
                await process_feed_group(http_session, feed_url, mags_in_group)

    await engine.dispose()
    logger.info("üèÅ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã ChromaDB –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")


if __name__ == "__main__":
    asyncio.run(run_update_cycle())